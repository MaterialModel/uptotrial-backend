---
description: Guidelines for OpenAI Responses API integration
type: ContextSpecific
---

# LLM Integration Guidelines

UpToTrial integrates with OpenAI's Responses API for natural language understanding. Follow these guidelines when working with the LLM components.

## Using the Responses Client

Always use the existing `ResponsesClient` from `src/uptotrial/infrastructure/llm/client.py`:

```python
from uptotrial.infrastructure.llm.client import ResponsesClient
from uptotrial.core.config import get_settings

# Initialization
settings = get_settings()
client = ResponsesClient(settings)

# Basic usage
response = await client.create_response(
    input_text="What trials are available for breast cancer?",
    temperature=0.3,  # Lower temperature for factual responses
)

# Search-specific usage
search_results = await client.search_clinical_trials(
    query="Phase 3 trials for lung cancer in New York",
    format_type="json",  # For structured data
)
```

## Error Handling

Always handle potential LLM errors gracefully:

```python
from uptotrial.api.errors import LLMError

try:
    response = await client.create_response(input_text=query)
    return process_response(response)
except LLMError as e:
    # Log the error with correlation ID
    logger.error(f"LLM error: {str(e)}", extra={"correlation_id": correlation_id})
    # Return a fallback response or re-raise as appropriate
    raise ServiceError(f"Error processing natural language query: {str(e)}")
```

## Testing LLM Code

When testing code that uses the LLM client, always mock the responses and run tests using tox:

```python
@pytest.fixture
def mock_llm_response():
    """Create a mock LLM response."""
    return {
        "message": {
            "content": "This is a mocked response",
        },
        "conversation_id": "test-conversation-id",
    }

@pytest.mark.asyncio
async def test_search_service(mock_llm_response):
    """Test search service with mocked LLM."""
    with patch("uptotrial.infrastructure.llm.client.ResponsesClient.create_response") as mock_create:
        mock_create.return_value = mock_llm_response
        service = SearchService()
        result = await service.search("test query")
        assert result is not None
```

Run LLM tests using tox:

```bash
# Run all LLM tests
tox -e specific -- tests/unit/test_llm/

# Run with coverage
tox -e coverage -- tests/unit/test_llm/

# Run specific test file
tox -e specific -- tests/unit/test_llm/test_client.py::test_search_clinical_trials
```

## Prompt Design

When designing prompts for the Responses API:

1. **Be Specific**: Clearly state what you want the model to do
2. **Provide Context**: Give necessary background information
3. **Structural Guidance**: Specify desired output format
4. **Examples**: Include examples when appropriate

Example system message:

```python
system_message = """
You are a clinical trials search assistant. Your task is to interpret natural language 
queries about clinical trials and extract key search parameters.

For each query, identify:
1. Condition/disease
2. Location (if specified)
3. Phase (if specified)
4. Patient characteristics (age, gender, etc.)
5. Other criteria (e.g., recruiting status)

Format your response as a JSON object with these fields.
"""
```

## Response Processing

When processing responses:

1. **Validation**: Always validate the response structure
2. **Graceful Degradation**: Have fallbacks for unexpected responses
3. **Conversation Management**: Store and use conversation IDs for context

```python
if "results" not in response:
    # Handle unexpected response format
    logger.warning(
        "Unexpected LLM response format", 
        extra={"correlation_id": correlation_id, "response": response}
    )
    return default_response()
```

## Conversation State

When using stateful conversations:

1. **Store conversation IDs**: Associate with user or session
2. **Context Window**: Be mindful of token limits in conversations
3. **Conversation Expiry**: Implement expiration for old conversations